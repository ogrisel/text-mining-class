{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining of BBC News Data\n",
    "\n",
    "## Part 2: Bag-of-Words Text Vectorization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Analyzer Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"C'est l'été au Brésil!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_analyzer = CountVectorizer().build_analyzer()\n",
    "word_analyzer(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_analyzer = CountVectorizer(strip_accents=\"unicode\", ngram_range=(2, 2)).build_analyzer()\n",
    "word_analyzer(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_analyzer = CountVectorizer(strip_accents=\"unicode\", ngram_range=(1, 2)).build_analyzer()\n",
    "word_analyzer(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_analyzer = CountVectorizer(ngram_range=(2, 2)).build_analyzer()\n",
    "word_analyzer(test_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzer = Preprocessor + Tokenizer (+ Token Filtering) (+ n-grams extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(strip_accents=\"unicode\", lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"C'est l'été au Brésil!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "- Type `vectorizer.build_<TAB>` to see the list of methods of the vectorizer object;\n",
    "\n",
    "- Use the vectorizer to build a preprocessor object and apply it to the test sentence: which transformations do you notice?\n",
    "\n",
    "- Use the vectorizer to build a tokenizer object and apply it to the preprocessed test sentence from the previous step;\n",
    "\n",
    "- Compare the results of the previous two steps with the output of the analyzer applied to the original test sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load notebook_solutions/build_preprocessor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load notebook_solutions/build_tokenizer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization of a Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "bbc_folder_path = Path(\"bbc\")\n",
    "text_filepaths = sorted(bbc_folder_path.glob(\"*/*.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of decoding the text manually as we did before, we will path the filenames directly to the vectorizer and let it decode using the encoding of our choice and ignore decoding errors (as we did previously):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "vectorizer = CountVectorizer(encoding=\"utf-8\", input=\"filename\",\n",
    "                             decode_error=\"ignore\")\n",
    "\n",
    "vectorizer.fit(text_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_items = sorted(vectorizer.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_items[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_items[5000:5010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_items[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_docs = vectorizer.transform(text_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_doc = vectorized_docs[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_doc.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: why does the vectorizer return a sparse matrix instead of a regular (dense) array?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_doc.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the indices of the non-zero values in the vectorized first document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_doc.indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dimension corresponds to a word in the vocabulary. We can retrieve the \"feature name\" for a specific dimension number as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "feature_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names[5360]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible to inverse a full vectorized document at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.inverse_transform(first_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Print the text of the original first document. And compare to the \"inversed transformed\" document (can you find the same words?):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**:\n",
    "\n",
    "- Why do you thing the result of the text vectorization procedure is called \"Bag-of-Words\" representation?\n",
    "\n",
    "- What as the main limitation of the \"Bag-of-Words\" representation?\n",
    "\n",
    "- Can you come up with a pair of sentence with completely different meanings but that would share the same Bag of Words vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Normalization\n",
    "\n",
    "- TF-IDF stands for **\"Term Frequency\" - \"Inverse Document Frequency\"**.\n",
    "\n",
    "- **Term Frequency** is the number of times a term or token appears in a given document;\n",
    "\n",
    "- **Document Frequency** is the number of times a term or token appears across all the documents of the corpus.\n",
    "\n",
    "\n",
    "**Note**: depending on the context, frequencies can either be:\n",
    "\n",
    "- **absolute** values (**integer counts**) or\n",
    "- **relative** values (**floating point numbers between 0 and 1** for **ratio** between two quantities).\n",
    "\n",
    "\n",
    "TF-IDF is a normalization procedures that transforms integer term frequency counts into a reweights term frequencies such that very frequent words (such as \"the\", \"a\", \"and\", \"it\"... in English) have lower importance than rarer words.\n",
    "\n",
    "The general idea, is that if we note:\n",
    "\n",
    "- $n_{t, d}$ the absolute term frequency (the number of times terms $t$ appears in document $d$);\n",
    "\n",
    "- $N_D$ the total number of documents in the dataset $D$;\n",
    "\n",
    "- $n_{t}$ the number of documents in dataset $D$ that contain the term $t$;\n",
    "\n",
    "The **term frequency** component can either be defined as:\n",
    "\n",
    "- $\\mathrm{tf} (t,d) = n_{t, d}$\n",
    "\n",
    "or my taking a subliner function of the counts such as:\n",
    "\n",
    "- $\\mathrm{tf} (t,d) = log(1 + n_{t, d})$\n",
    "\n",
    "Then the raw **inverse document frequency** is:\n",
    "\n",
    "- $\\mathrm{idf} (t) = \\frac{N}{n_{t}}$\n",
    "\n",
    "In practice one often takes the log of this quantity and define:\n",
    "\n",
    "- $\\mathrm{idf} (t, D) = log(\\frac{N_D}{n_{t}})$\n",
    "\n",
    "The final TF-IDF value is computed by taking the product of those two quantities:\n",
    "\n",
    "$$\\mathrm{tfidf} (t,d,D) = \\mathrm{tf} (t,d)\\cdot \\mathrm{idf} (t,D)$$\n",
    "\n",
    "There are many variants and the one implemented in scikit-learn is not necessarily the most standard (for historical reasons). The Wikipedia article has the list of variants:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Tf–idf\n",
    "\n",
    "The scikit-learn documentation has comprehensive information on the vectorizers of scikit-learn and the TF-IDF variant implemented there:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_docs = tfidf_transformer.fit_transform(vectorized_docs)\n",
    "tfidf_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_doc.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_tfidf_doc = tfidf_docs[0, :]\n",
    "first_tfidf_doc.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer.idf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises**:\n",
    "    \n",
    "Using the `vectorizer.get_feature_names()` vector and the `tfidf_transformer.idf_`, compute the list of the top 10 least and most informative words in the BBC corpus.\n",
    "\n",
    "Hint: you can put the two vectors in a `pandas.DataFrame` and use the [nlargest](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.nlargest.html) and [nsmallest](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.nsmallest.html) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load notebook_solutions/top_idf.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "- Why do the top 10 IDF-weighted terms have the same IDF value in your opinion?\n",
    "\n",
    "- What is the problem with using such terms in a text classification scenario?\n",
    "\n",
    "- What do you think about the top 10 lowest? What could we do to improve the computational performance of a text classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing and Weighting at Once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(encoding=\"utf-8\", input=\"filename\",\n",
    "                                   decode_error=\"ignore\",\n",
    "                                   min_df=5, max_df=0.8)\n",
    "\n",
    "tfidf_docs_trimmed = tfidf_vectorizer.fit_transform(text_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_docs_trimmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "    \n",
    "- What do you notice about the shape of this new vectorized text corpus (compared to the previous version)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "weighted_terms_trimmed = pd.DataFrame({\n",
    "    \"term\": tfidf_vectorizer.get_feature_names(),\n",
    "    \"idf\": tfidf_vectorizer.idf_})\n",
    "\n",
    "print(\"Most 'informative' terms:\")\n",
    "print(weighted_terms_trimmed.nlargest(10, \"idf\"), end=\"\\n\\n\")\n",
    "\n",
    "print(\"Least informative terms:\")\n",
    "print(weighted_terms_trimmed.nsmallest(10, \"idf\"), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Similarities in TF-IDF Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common way to measure the semantic similarity of two documents is to compute the cosine similarity of their TF-IDF vectors:\n",
    "\n",
    "\n",
    "$$cosine(x_1, x_2) = \\frac{x_1 \\cdot x_2}{||x_1|| ||x_2||}$$\n",
    "\n",
    "\n",
    "**Question**\n",
    "\n",
    "- Show that if two vectors have unit norms, then maximizing their cosine similarity is equivalent to minimizing their euclidean distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_docs.multiply(vectorized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sparse_row_norms(data):\n",
    "    return np.asarray((data.multiply(data)).sum(axis=1)).ravel()\n",
    "\n",
    "\n",
    "sparse_row_norms(vectorized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_row_norms(tfidf_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_row_norms(tfidf_docs_trimmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "- Why do all TF-IDF vectorized documents have unit norm? Hint: look at the default parameters for thoe classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_dot_products(query_vector, other_vectors):\n",
    "    dot_products = query_vector.multiply(other_vectors).sum(axis=1)\n",
    "    return np.asarray(dot_products).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = sparse_dot_products(tfidf_docs[0, :], tfidf_docs[1:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [path.parent.name for path in text_filepaths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises**:\n",
    "- Find the document id and category for the 15 documents that are most similar to the first document in the corpus.\n",
    "- Print the text of the first document (the query) and the text of the most similar documnt from your computed list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load notebook_solutions/tfidf_similarities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of TF-IDF Weighting on K-NN Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "n_neighbors = 5\n",
    "cv_scores = cross_val_score(KNeighborsClassifier(n_neighbors=n_neighbors),\n",
    "                            vectorized_docs, categories, cv=10)\n",
    "\n",
    "print(f\"Val. accuracy: {cv_scores.mean():.3f} (+/-{cv_scores.std():.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cv_scores = cross_val_score(KNeighborsClassifier(n_neighbors=n_neighbors),\n",
    "                            tfidf_docs, categories, cv=10)\n",
    "\n",
    "print(f\"Val. accuracy: {cv_scores.mean():.3f} (+/-{cv_scores.std():.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cv_scores = cross_val_score(KNeighborsClassifier(n_neighbors=n_neighbors),\n",
    "                            tfidf_docs_trimmed, categories, cv=10)\n",
    "\n",
    "print(f\"Val. accuracy: {cv_scores.mean():.3f} (+/-{cv_scores.std():.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "\n",
    "- Why is K-NN classification accuracy better on tf-idf vectors than on pure tf vectors (count vectors)?\n",
    "\n",
    "- Why is K-NN classification speed is faster on trimmed tf-idf vectors than on the original tf-idf vectors?\n",
    "\n",
    "- Why is K-NN classification accuracy better on trimmed tf-idf vectors than on the original tf-idf vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.7",
   "language": "python",
   "name": "python-3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
